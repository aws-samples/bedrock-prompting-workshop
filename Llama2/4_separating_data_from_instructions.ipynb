{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b010b61e-a569-43e2-97cd-e9798e79905d",
   "metadata": {},
   "source": [
    "# **Chapter 4: Separating Data from Instructions**\n",
    "\n",
    "---\n",
    "**Lesson:**\n",
    "\n",
    "When using Llama2, you might not always want to write out full prompts from scratch. Instead, you can create prompt templates that you can later customize with specific data inputs before submitting to Llama2. This method is particularly useful when you want Llama2 to perform a consistent task, but with different data each time.\n",
    "\n",
    "Creating prompt templates involves defining a fixed structure and identifying parts that can be replaced with variable user input. By doing this, you can quickly and efficiently generate customized prompts that Llama2 can execute.\n",
    "\n",
    "In the following steps, we'll show you how to design a flexible prompt template and how to seamlessly integrate user-specific data into your prompts for Llama2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5062f18e-2715-463a-9db5-ca7f3f749f05",
   "metadata": {},
   "source": [
    "First we will setup our dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4365f02-24a2-4b3c-ac00-80e9030241f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#Install dependencies\n",
    "%pip install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\"\n",
    "\n",
    "%pip install --quiet langchain==0.0.304\n",
    "\n",
    "#Import libraries, and set up Bedrock client\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")\n",
    "\n",
    "modelId = 'meta.llama2-13b-chat-v1' # change this to use a different version from the model provider\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'\n",
    "outputText = \"\\n\"\n",
    "\n",
    "def invoke_model_and_get_response(prompt_data): \n",
    "    body = json.dumps({ \n",
    "        'prompt': prompt_data,\n",
    "        'max_gen_len': 512,\n",
    "        'top_p': 0.9,\n",
    "        'temperature': 0.2\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        response = boto3_bedrock.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "        response_body = json.loads(response.get('body').read().decode('utf-8'))\n",
    "        outputText = response_body['generation'].strip()\n",
    "        return outputText\n",
    "\n",
    "    except botocore.exceptions.ClientError as error:\n",
    "        if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "            return (f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "                    \\nTo troubleshoot this issue please refer to the following resources.\\\n",
    "                     \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "                     \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "        else:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5502384f-e01e-4249-824c-5825d67f13ea",
   "metadata": {},
   "source": [
    "# **Examples:**\n",
    "\n",
    "In this first example (4.1), we're asking Llama2 to act as an animal noise generator. Notice that the full prompt submitted to Llama2 is just the prompt template substituted with the input (in this case, \"Cow\"). Notice that the word \"Cow\" replaces the \"{ANIMAL}\" section.\n",
    "\n",
    "> *Note: You don't have to call your substitution placeholder anything in particular in practice. For this example, definitely use {ANIMAL}, as that's how the exercise is formatted. But in general, just as easily, we could have called it \"{CREATURE}\" or \"{A}\" (but it's generally good to have your stand-ins be specific and relevant so that your prompt is easy to understand even without the substitution). Just make sure that whatever you name your substitution placeholder is what you use for the substitution formula.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9156623a-8f6a-44de-93b9-156acc7a69ef",
   "metadata": {},
   "source": [
    "**Example 4.1 - Moo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb1c33-e5c2-440e-a76e-e81f0530589c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "# Create a prompt template that has input variables\n",
    "multi_var_prompt = PromptTemplate(\n",
    "    input_variables=[\"ANIMAL\"], \n",
    "    template=\"\"\"\n",
    "    <s>[INST] \n",
    "    \n",
    "    <<SYS>>\n",
    "    <</SYS>>\n",
    "    \n",
    "    I will tell you the name of an animal. Please respond with the noise that animal makes. {ANIMAL}.[/INST]\"\"\"\n",
    ")\n",
    "\n",
    "prompt = multi_var_prompt.format(ANIMAL=\"Cow\")\n",
    "    \n",
    "response = invoke_model_and_get_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56845631-6bf2-4bc8-ae8c-3b61024b389a",
   "metadata": {},
   "source": [
    "**Example 4.2 - This could have been an email**\n",
    "\n",
    "Why would we want to separate and substitute inputs like this? Well, prompt templates simplify repetitive tasks. Let's say you build a prompt structure that invites third party users to submit content to the prompt (in the previous example the animal whose sound they want to generate). These third party users don't have to write or even see the full prompt. All they have to do is fill in variables.\n",
    "\n",
    "We do this substitution here leverage an open source Python library called Langchain.  We use the {curly-brackets} formatting in our own code.\n",
    "\n",
    "> *Note: Prompt templates can have as many variables as desired.*\n",
    "\n",
    "When introducing substitution variables like this, it is very important to make sure Llama2 knows where variables start and end (vs. instructions or task descriptions). Let's look at an example where there is no separation between the instructions and the substitution variable.\t\t\n",
    "\n",
    "> *Note: We recommend that you use specifically XML tags as separators for Llama2, as opposed to some other separator, as Llama2's already trained to recognize XML tags in particular.\"*\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819ec73-71a3-4d37-96d2-7e0d1504bbea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a prompt template that has input variables\n",
    "multi_var_prompt = PromptTemplate(\n",
    "    input_variables=[\"EMAIL\"], \n",
    "    template=\"\"\"\n",
    "    <s>[INST] \n",
    "    \n",
    "    <<SYS>>\n",
    "    <</SYS>>\n",
    "    \n",
    "    Hey. Make this email more polite. <email> {EMAIL} </email>[/INST]\"\"\"\n",
    ")\n",
    "\n",
    "prompt = multi_var_prompt.format(EMAIL=\"Show up at 6am because I'm the CEO and I say so\")\n",
    "    \n",
    "response = invoke_model_and_get_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24323c14-38c0-47ea-a657-21f8f2b8cbcc",
   "metadata": {},
   "source": [
    "**Example 4.3 - The Farm**\n",
    "\n",
    "Let's see another example of how XML tags can help us. \n",
    "We need to surround the user input sentences in XML tags. This shows Llama2 where the input data begins and ends despite the misleading hyphen before \"Each is about an animal, like rabbits.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e06b03f-1b01-4f68-99a0-344db3f0ed8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a prompt template that has input variables\n",
    "multi_var_prompt = PromptTemplate(\n",
    "    input_variables=[\"SENTENCES\"], \n",
    "    template=\"\"\"\n",
    "    <s>[INST] \n",
    "    \n",
    "    <<SYS>>\n",
    "    <</SYS>>\n",
    "    \n",
    "    I will give you a list of sentences. Each is about an animal, like rabbits. Tell me the second on the list. <sentences> {SENTENCES} </sentences>[/INST]\"\"\"\n",
    ")\n",
    "\n",
    "prompt = multi_var_prompt.format(SENTENCES=\"\"\"\n",
    "-I like how cows sound,\n",
    "-This sentence may appear to be about dogs but it's actually about pigs,\n",
    "-This sentence is about spiders\"\"\")\n",
    "    \n",
    "response = invoke_model_and_get_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b39fef1-843f-4f29-a2dc-425b7eea9c93",
   "metadata": {},
   "source": [
    "# **Exercises**\n",
    "\n",
    "The following exercises will need you to manipulate the prompt to get the desired output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df73c912-13d3-4e06-a7ac-a5cd46e9def3",
   "metadata": {},
   "source": [
    "**Excercise 4.1 - Haiku Topic**\n",
    "\n",
    "Write a prompt in the highlighted template box that will take in a variable called \"{TOPIC}\" and output a haiku about the topic.\n",
    "\n",
    "Remember to format everything properly, the way we covered in earlier lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aebc245-9670-44b0-b8d1-61ee2cc9da42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a prompt template that has input variables\n",
    "multi_var_prompt = PromptTemplate(\n",
    "    input_variables=[\"TOPIC\"], \n",
    "    template=\"\"\"\n",
    "    <s>[INST] \n",
    "    \n",
    "    <<SYS>>\n",
    "    <</SYS>>\n",
    "    \n",
    "    I will tell you a topic. Please write a haiku on that topic. <topic> {TOPIC} </topic>[/INST]\"\"\"\n",
    ")\n",
    "\n",
    "prompt = multi_var_prompt.format(TOPIC=\"Football\")\n",
    "    \n",
    "response = invoke_model_and_get_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6f0f1a-04a4-47e7-950a-dfdd86c1e4df",
   "metadata": {},
   "source": [
    "**Excercise 4.2 - Dog Question With Typos**\n",
    "\n",
    "Fix the prompt in the template below by adding XML tags, or making small edits so that Llama2 produces the right answer.\n",
    "\n",
    "Try not to change anything else about the prompt. The messy and mistake-ridden writing is intentional, so you can see how Llama2 reacts to such mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537b39f-5753-4fd3-a0ad-06ad7db553a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a prompt template that has input variables\n",
    "multi_var_prompt = PromptTemplate(\n",
    "    input_variables=[\"QUESTION\"], \n",
    "    template=\"\"\"\n",
    "    <s>[INST] \n",
    "    \n",
    "    <<SYS>>\n",
    "    You are a helpful, respectful and honest assistant. \n",
    "    If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "    <</SYS>>\n",
    "    \n",
    "    hia its me i have a q about dogs jkaerjv <question> {QUESTION} </question> jklmvca tx it help me muhch much atx fst fst answer short short tx [/INST]\"\"\"\n",
    ")\n",
    "\n",
    "prompt = multi_var_prompt.format(QUESTION=\"are cats brown?\")\n",
    "    \n",
    "response = invoke_model_and_get_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e646003-3930-4d64-a340-1b56bf5b4eb6",
   "metadata": {},
   "source": [
    "# Chapter 4 - END."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
